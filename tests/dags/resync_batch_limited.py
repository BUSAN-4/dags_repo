"""
Airflow DAG: ê¸°ì¡´ ë°ì´í„° ì¬ë™ê¸°í™” (1ë¶„ ë‹¨ìœ„ + 5ê°œì”©)
- ê° 1ë¶„ ì‹œê°„ ë²”ìœ„ì— ëŒ€í•´ 5ê°œì”© ë°°ì¹˜ë¡œ RDS -> Kafka ì „ì†¡
"""
from datetime import datetime, timedelta
from airflow import DAG
from airflow.decorators import task
from airflow.operators.python import PythonOperator
import requests
import time
import logging

default_args = {
    'owner': 'airflow',
    'depends_on_past': False,
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
}

# Flink SQL Gateway ì„¤ì •
FLINK_SQL_GATEWAY_URL = "http://flink-sql-gateway-20.flink.svc.cluster.local:8083"
SQL_FILE_PATH = "/opt/airflow/dags/flink_sql/04_resync_batch_limited.sql"

with DAG(
    'resync_batch_limited',
    default_args=default_args,
    description='ê¸°ì¡´ ë°ì´í„° ì¬ë™ê¸°í™” (1ë¶„ ë‹¨ìœ„ + 5ê°œì”©)',
    schedule=None,  # ìˆ˜ë™ ì‹¤í–‰
    start_date=datetime(2024, 1, 1),
    catchup=False,
    tags=['flink', 'batch', 'resync', 'limited'],
) as dag:

    @task
    def read_sql_file():
        """SQL íŒŒì¼ ì½ê¸°"""
        with open(SQL_FILE_PATH, 'r', encoding='utf-8') as f:
            sql_content = f.read()
        logging.info(f"âœ… SQL íŒŒì¼ ì½ê¸° ì™„ë£Œ: {SQL_FILE_PATH}")
        return sql_content

    @task
    def calculate_batches(**context):
        """
        ì²˜ë¦¬í•  ì‹œê°„ ë²”ìœ„ ë° ë°°ì¹˜ ê°œìˆ˜ ê³„ì‚°
        - 1ë¶„ ë‹¨ìœ„ ì‹œê°„ ë²”ìœ„
        - ê° ì‹œê°„ ë²”ìœ„ë§ˆë‹¤ offsetì„ 5ì”© ì¦ê°€ì‹œí‚¤ë©° ì²˜ë¦¬
        """
        # DAG Run Confì—ì„œ íŒŒë¼ë¯¸í„° ê°€ì ¸ì˜¤ê¸°
        conf = context['dag_run'].conf or {}
        start_time_str = conf.get('start_time', '2024-12-01 00:00:00')
        end_time_str = conf.get('end_time', '2024-12-01 00:01:00')
        max_batches_per_minute = conf.get('max_batches_per_minute', 20)  # ìµœëŒ€ 100ê°œ(5*20)
        
        logging.info(f"ğŸ“… ì‹œê°„ ë²”ìœ„: {start_time_str} ~ {end_time_str}")
        logging.info(f"ğŸ“¦ ìµœëŒ€ ë°°ì¹˜ ìˆ˜ (1ë¶„ë‹¹): {max_batches_per_minute}")
        
        # 1ë¶„ ë‹¨ìœ„ë¡œ ë‚˜ëˆ”
        start_dt = datetime.strptime(start_time_str, '%Y-%m-%d %H:%M:%S')
        end_dt = datetime.strptime(end_time_str, '%Y-%m-%d %H:%M:%S')
        
        time_ranges = []
        current_time = start_dt
        
        while current_time < end_dt:
            next_time = current_time + timedelta(minutes=1)
            if next_time > end_dt:
                next_time = end_dt
            
            # ê° 1ë¶„ ë²”ìœ„ë§ˆë‹¤ offsetì„ 0, 5, 10, ..., (max_batches_per_minute-1)*5 ê¹Œì§€
            for batch_idx in range(max_batches_per_minute):
                offset = batch_idx * 5
                time_ranges.append({
                    'start_time': current_time.strftime('%Y-%m-%d %H:%M:%S'),
                    'end_time': next_time.strftime('%Y-%m-%d %H:%M:%S'),
                    'offset': offset,
                    'batch_name': f"{current_time.strftime('%Y%m%d_%H%M')}_offset_{offset}"
                })
            
            current_time = next_time
        
        logging.info(f"âœ… ì´ {len(time_ranges)}ê°œ ë°°ì¹˜ ìƒì„±")
        return time_ranges

    @task
    def submit_batch_job(sql_content: str, batch_info: dict):
        """
        Flink SQL Gatewayë¡œ ë°°ì¹˜ ì‘ì—… ì œì¶œ
        """
        start_time = batch_info['start_time']
        end_time = batch_info['end_time']
        offset = batch_info['offset']
        batch_name = batch_info['batch_name']
        
        logging.info(f"ğŸš€ ë°°ì¹˜ ì‹œì‘: {batch_name}")
        logging.info(f"   ì‹œê°„: {start_time} ~ {end_time}")
        logging.info(f"   Offset: {offset} (í–‰ {offset+1}~{offset+5})")
        
        # ì„¸ì…˜ ìƒì„±
        session_resp = requests.post(
            f"{FLINK_SQL_GATEWAY_URL}/v1/sessions",
            json={"properties": {"execution.runtime-mode": "batch"}}
        )
        session_resp.raise_for_status()
        session_handle = session_resp.json()['sessionHandle']
        logging.info(f"ğŸ”‘ ì„¸ì…˜ ìƒì„±: {session_handle}")
        
        try:
            # SQL íŒŒì‹± (ì£¼ì„ ì œê±° ë° ì„¸ë¯¸ì½œë¡ ìœ¼ë¡œ ë¶„ë¦¬)
            statements = []
            current_statement = ""
            
            for line in sql_content.split('\n'):
                line = line.strip()
                if line.startswith('--') or not line:
                    continue
                current_statement += line + " "
                if line.endswith(';'):
                    statements.append(current_statement.strip())
                    current_statement = ""
            
            # ê° SQLë¬¸ ì‹¤í–‰
            for idx, stmt in enumerate(statements, 1):
                # íŒŒë¼ë¯¸í„° ì¹˜í™˜
                stmt = stmt.replace(':start_time', f"'{start_time}'")
                stmt = stmt.replace(':end_time', f"'{end_time}'")
                stmt = stmt.replace(':offset', str(offset))
                
                logging.info(f"[{idx}/{len(statements)}] SQL ì‹¤í–‰ ì¤‘...")
                
                # SQL ì‹¤í–‰
                exec_resp = requests.post(
                    f"{FLINK_SQL_GATEWAY_URL}/v1/sessions/{session_handle}/statements",
                    json={"statement": stmt}
                )
                exec_resp.raise_for_status()
                operation_handle = exec_resp.json()['operationHandle']
                
                # ì™„ë£Œ ëŒ€ê¸° (INSERTëŠ” FINISHED ìƒíƒœê¹Œì§€ ê¸°ë‹¤ë¦¼)
                if stmt.strip().upper().startswith('INSERT'):
                    max_wait = 300  # ìµœëŒ€ 5ë¶„
                    waited = 0
                    while waited < max_wait:
                        status_resp = requests.get(
                            f"{FLINK_SQL_GATEWAY_URL}/v1/sessions/{session_handle}/operations/{operation_handle}/status"
                        )
                        status = status_resp.json().get('status')
                        
                        if status == 'FINISHED':
                            logging.info(f"âœ… [{idx}/{len(statements)}] ì™„ë£Œ!")
                            break
                        elif status == 'ERROR':
                            error_msg = status_resp.json().get('error', 'Unknown error')
                            raise Exception(f"SQL ì‹¤í–‰ ì‹¤íŒ¨: {error_msg}")
                        
                        time.sleep(2)
                        waited += 2
                    
                    if waited >= max_wait:
                        logging.warning(f"âš ï¸ [{idx}/{len(statements)}] íƒ€ì„ì•„ì›ƒ (5ë¶„ ì´ˆê³¼)")
                else:
                    # CREATE TABLE ë“±ì€ ì¦‰ì‹œ ì™„ë£Œë¡œ ê°„ì£¼
                    time.sleep(0.5)
                    logging.info(f"âœ… [{idx}/{len(statements)}] ì™„ë£Œ!")
            
            logging.info(f"âœ… ë°°ì¹˜ ì™„ë£Œ: {batch_name}")
            
        finally:
            # ì„¸ì…˜ ì¢…ë£Œ
            try:
                requests.delete(f"{FLINK_SQL_GATEWAY_URL}/v1/sessions/{session_handle}")
                logging.info(f"ğŸ”’ ì„¸ì…˜ ì¢…ë£Œ: {session_handle}")
            except Exception as e:
                logging.warning(f"ì„¸ì…˜ ì¢…ë£Œ ì‹¤íŒ¨: {e}")

    # Task ì‹¤í–‰ ìˆœì„œ
    sql_content = read_sql_file()
    batches = calculate_batches()
    
    # ê° ë°°ì¹˜ë¥¼ ìˆœì°¨ ì‹¤í–‰ (ë™ì  íƒœìŠ¤í¬ ë§¤í•‘)
    submit_batch_job.expand(
        sql_content=[sql_content] * 1,  # ëª¨ë“  ë°°ì¹˜ì— ë™ì¼í•œ SQL ì „ë‹¬
        batch_info=batches
    )

