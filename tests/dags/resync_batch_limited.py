"""
Airflow DAG: ê¸°ì¡´ ë°ì´í„° ì¬ë™ê¸°í™” (5ê°œì”© ë°°ì¹˜)
- ì „ì²´ ì‹œê°„ ë²”ìœ„ì—ì„œ 5ê°œì”© ìˆœì°¨ì ìœ¼ë¡œ RDS -> Kafka ì „ì†¡
"""
from datetime import datetime, timedelta
from airflow import DAG
from airflow.decorators import task
from airflow.operators.python import PythonOperator
import requests
import time
import logging

default_args = {
    'owner': 'airflow',
    'depends_on_past': False,
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
}

# Flink SQL Gateway ì„¤ì •
FLINK_SQL_GATEWAY_URL = "http://flink-sql-gateway-20.flink.svc.cluster.local:8083"
SQL_FILE_PATH = "/opt/airflow/dags/flink_sql/04_resync_batch_limited.sql"

with DAG(
    'resync_batch_limited',
    default_args=default_args,
    description='ê¸°ì¡´ ë°ì´í„° ì¬ë™ê¸°í™” (5ê°œì”© ë°°ì¹˜)',
    schedule='*/1 * * * *',  # ë§¤ë¶„ ì‹¤í–‰
    start_date=datetime(2024, 1, 1),
    catchup=False,
    tags=['flink', 'batch', 'resync', 'limited'],
) as dag:

    @task
    def read_sql_file():
        """SQL íŒŒì¼ ì½ê¸°"""
        with open(SQL_FILE_PATH, 'r', encoding='utf-8') as f:
            sql_content = f.read()
        logging.info(f"âœ… SQL íŒŒì¼ ì½ê¸° ì™„ë£Œ: {SQL_FILE_PATH}")
        return sql_content

    @task
    def calculate_batches(**context):
        """
        ì²˜ë¦¬í•  ë°°ì¹˜ ê°œìˆ˜ ê³„ì‚°
        - ì‹œê°„ ë²”ìœ„ëŠ” ê³ ì • (start_time ~ end_time)
        - offsetë§Œ 5ì”© ì¦ê°€ì‹œí‚¤ë©° 5ê°œì”© ì²˜ë¦¬
        """
        # DAG Run Confì—ì„œ íŒŒë¼ë¯¸í„° ê°€ì ¸ì˜¤ê¸°
        conf = context['dag_run'].conf or {}
        start_time = conf.get('start_time', '2024-12-01 00:00:00')
        end_time = conf.get('end_time', '2024-12-12 00:00:00')
        max_batches = conf.get('max_batches', 100)  # ê¸°ë³¸ 100ê°œ ë°°ì¹˜ (ì´ 500ê°œ í–‰)
        
        logging.info(f"ğŸ“… ì‹œê°„ ë²”ìœ„: {start_time} ~ {end_time} (ê³ ì •)")
        logging.info(f"ğŸ“¦ ì´ ë°°ì¹˜ ìˆ˜: {max_batches} (ì´ {max_batches * 5}ê°œ í–‰)")
        
        batches = []
        for batch_idx in range(max_batches):
            offset = batch_idx * 5
            batches.append({
                'start_time': start_time,  # í•­ìƒ ë™ì¼
                'end_time': end_time,      # í•­ìƒ ë™ì¼
                'offset': offset,          # 0, 5, 10, 15, 20, ...
                'batch_name': f"batch_{batch_idx:04d}_offset_{offset}"
            })
        
        logging.info(f"âœ… ì´ {len(batches)}ê°œ ë°°ì¹˜ ìƒì„±")
        return batches

    @task
    def submit_batch_job(sql_content: str, batch_info: dict):
        """
        Flink SQL Gatewayë¡œ ë°°ì¹˜ ì‘ì—… ì œì¶œ
        """
        start_time = batch_info['start_time']
        end_time = batch_info['end_time']
        offset = batch_info['offset']
        batch_name = batch_info['batch_name']
        
        logging.info(f"ğŸš€ ë°°ì¹˜ ì‹œì‘: {batch_name}")
        logging.info(f"   ì‹œê°„: {start_time} ~ {end_time}")
        logging.info(f"   Offset: {offset} (í–‰ {offset+1}~{offset+5})")
        
        # ì„¸ì…˜ ìƒì„±
        session_resp = requests.post(
            f"{FLINK_SQL_GATEWAY_URL}/v1/sessions",
            json={"properties": {"execution.runtime-mode": "batch"}}
        )
        session_resp.raise_for_status()
        session_handle = session_resp.json()['sessionHandle']
        logging.info(f"ğŸ”‘ ì„¸ì…˜ ìƒì„±: {session_handle}")
        
        try:
            # SQL íŒŒì‹± (ì£¼ì„ ì œê±° ë° ì„¸ë¯¸ì½œë¡ ìœ¼ë¡œ ë¶„ë¦¬)
            statements = []
            current_statement = ""
            
            for line in sql_content.split('\n'):
                line = line.strip()
                if line.startswith('--') or not line:
                    continue
                current_statement += line + " "
                if line.endswith(';'):
                    statements.append(current_statement.strip())
                    current_statement = ""
            
            # ê° SQLë¬¸ ì‹¤í–‰
            for idx, stmt in enumerate(statements, 1):
                # íŒŒë¼ë¯¸í„° ì¹˜í™˜
                stmt = stmt.replace(':start_time', f"'{start_time}'")
                stmt = stmt.replace(':end_time', f"'{end_time}'")
                stmt = stmt.replace(':offset', str(offset))
                
                logging.info(f"[{idx}/{len(statements)}] SQL ì‹¤í–‰ ì¤‘...")
                
                # SQL ì‹¤í–‰
                exec_resp = requests.post(
                    f"{FLINK_SQL_GATEWAY_URL}/v1/sessions/{session_handle}/statements",
                    json={"statement": stmt}
                )
                exec_resp.raise_for_status()
                operation_handle = exec_resp.json()['operationHandle']
                
                # ì™„ë£Œ ëŒ€ê¸° (INSERTëŠ” FINISHED ìƒíƒœê¹Œì§€ ê¸°ë‹¤ë¦¼)
                if stmt.strip().upper().startswith('INSERT'):
                    max_wait = 300  # ìµœëŒ€ 5ë¶„
                    waited = 0
                    while waited < max_wait:
                        status_resp = requests.get(
                            f"{FLINK_SQL_GATEWAY_URL}/v1/sessions/{session_handle}/operations/{operation_handle}/status"
                        )
                        status = status_resp.json().get('status')
                        
                        if status == 'FINISHED':
                            logging.info(f"âœ… [{idx}/{len(statements)}] ì™„ë£Œ!")
                            break
                        elif status == 'ERROR':
                            error_msg = status_resp.json().get('error', 'Unknown error')
                            raise Exception(f"SQL ì‹¤í–‰ ì‹¤íŒ¨: {error_msg}")
                        
                        time.sleep(2)
                        waited += 2
                    
                    if waited >= max_wait:
                        logging.warning(f"âš ï¸ [{idx}/{len(statements)}] íƒ€ì„ì•„ì›ƒ (5ë¶„ ì´ˆê³¼)")
                else:
                    # CREATE TABLE ë“±ì€ ì¦‰ì‹œ ì™„ë£Œë¡œ ê°„ì£¼
                    time.sleep(0.5)
                    logging.info(f"âœ… [{idx}/{len(statements)}] ì™„ë£Œ!")
            
            logging.info(f"âœ… ë°°ì¹˜ ì™„ë£Œ: {batch_name}")
            
        finally:
            # ì„¸ì…˜ ì¢…ë£Œ
            try:
                requests.delete(f"{FLINK_SQL_GATEWAY_URL}/v1/sessions/{session_handle}")
                logging.info(f"ğŸ”’ ì„¸ì…˜ ì¢…ë£Œ: {session_handle}")
            except Exception as e:
                logging.warning(f"ì„¸ì…˜ ì¢…ë£Œ ì‹¤íŒ¨: {e}")

    # Task ì‹¤í–‰ ìˆœì„œ
    sql_content = read_sql_file()
    batches = calculate_batches()
    
    # ê° ë°°ì¹˜ë¥¼ ìˆœì°¨ ì‹¤í–‰ (ë™ì  íƒœìŠ¤í¬ ë§¤í•‘)
    submit_batch_job.expand(
        sql_content=[sql_content] * 1,  # ëª¨ë“  ë°°ì¹˜ì— ë™ì¼í•œ SQL ì „ë‹¬
        batch_info=batches
    )

